\subsection{Baseline}
As a baseline, we built with Keras a simple autoencoder having 8 Convolutional layers for the encoding part
(ReLU activations, zero-padding, $3\times3$ kernels and sometimes $2\times2$ strides), while the decoding part
consists in the combination of 5 Convolutional layers (ReLU activations except for the last layer, zero-padding
and $3\times3$ kernel) and 3 UpSampling layers of size $2\times2$. The encoder learns a compact representation
of the black and white input image and the decoder generates the corresponding novel coloured image.

The model was trained (50 epochs) on a heterogeneous dataset containing all the data available.

Moreover, we enriched this model with a novel approach: instead of using the original dataset, we fed the model
with the cartoonized (black and white) version of the images, computed with the pre-trained GAN-based cartoonization
model by \cite{cartoonize}. This cartoonization provides fine-grained results (we don't miss much information)
and synthesizes the original images in order to exclude noisy elements that could interfere with the colorization
task. The model produces cartoonized colored images whose $a$ and $b$ channels are combined with the $L$ channel
of the original \textit{Lab} images. Therefore, we mantain the original details of the pictures, while producing a
more precise and sectorial colorization.

For comparison, we also include in our experiments the Baseline without cartoonization (Baseline w/c).