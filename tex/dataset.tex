\section{Dataset}
We considered three types of images: 4023 originally colored images from five different datasets, 18 originally black and white images from various artists and 180 filtered images (see more details in Image filtering section) obtained starting from 18 originally colored images.

Our data includes heterogeneous images, representing many different environments, situations and subjects, coming from various sources:
\begin{itemize}
	\item a subset of ImageNet made of 12 classes (200 images each) taken from \cite{imagenette}, ten of which are easily classifiable classes (tench, English springer, cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball and parachute) while the other two are hard to classify (Samoyed and Rhodesian ridgeback);
	\item a subset of 100 randomly selected images from Pascal VOC \cite{pascal} representing realistic scenes in which the subjects could be animals, human beeings, plants, rooms, landscapes, various objects and vehicles;
	\item a subset of 200 randomly selected images form Places205 \cite{place} reguarding mountain, desert, sea, beach and island landscapes.
	\item a subset of Bird Species \cite{bird} made of 8 classes (100 images each), depicting birds with unusual colors (Cuban Tody, Fire Tailed Myzornis, Flamingo, Nicobar Pigeon and Pink Robin) and best known birds (Bald Eagle, Ostrich and Touchan);
	\item a subset of Flower Species \cite{flower} made of 6 classes (from 50 to 100 images each), depicting flowers with unusual colors and shapes (Purple Coneflower, Grape Hyacinth, Hibiscus) and best known flowers (Rose, Water Lily and Giant White Arum Lily).
\end{itemize}

The images have been preprocessed by using OpenCV (ChromaGAN and InstColorization) or Pillow combined with Skimage
(Baseline, Dahl, Zhang, Siggraph) and have been reshaped to various formats. Dahl also required center cropping and desaturation. Despite the preliminar reshape, Zhang, Siggraph and ChromaGAN provides for results having the original image shape.

Given an RGB image
we obtain the corrisponding image in the \textit{Lab} color space, in which images have 3 new
channels: $L$ for perceptual lightness ($L=0$ is white, $L=100$ is black), $a$ and $b$ for
four primary colors ($a=\pm100$ are red and green, $b=\pm100$ are yellow and blue). Our models get only the $L$ channel as input (greyscale images) with the goal of predicting the $a$ and $b$
channels.

In particular, the classification with AlexNet required the image normalization in the
range $[0,1]$ and a further standardization according to the mean and standard deviation of the
training set. On the other hand, the LPIPS metric required image normalization in the range $[-1,1]$.