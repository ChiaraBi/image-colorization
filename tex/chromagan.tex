\subsection{ChromaGAN}
The strength of ChromaGAN is to use the semantic understanding of the depicted scene combined with a generative adversarial network (GAN). In fact, the semantic class distribution learning makes ChromaGAN capable of variability (it can provides different colors for objects belonging to the same category, as it happens in reality) while the generative adversarial learning leads to vivid and vibrant colorizations.

The generator $\mathcal{G}_\theta$ is divided into two jointly trained subnetworks: the first one outputs the chrominance information $\mathcal{G}_{\theta_1}^1(L) = (a,b)$ (Figure \ref{fig:chr} in blue) and the second one is a classification network giving in output the class distribution vector $\mathcal{G}_{\theta_2}^2(L)=y$ (Figure \ref{fig:chr} in grey) that is trained to be close to the VGG-16 output, in order to generate useful information for the colorization process. The inital layers (Figure \ref{fig:chr} in yellow) are shared and initialized with the pre-trained VGG-16 weights. Then, both the subnetworks split into two tracks (Figure \ref{fig:chr} in purple and red for $\mathcal{G}_{\theta_1}^1$, and in red and grey for $\mathcal{G}_{\theta_2}^2$). The results are fused by concatenation and used to generate the colors.

The discriminator $\mathcal{D}_w$ focuses on the local patches of the generated image and classifies each of them as real or fake. The ultimate goal is to find the optimum of:
\begin{equation*}
	\min_{\mathcal{G}_\theta}\max_{\mathcal{D}_w} \mathcal{L}(\mathcal{G}_\theta, \mathcal{D}_w) = \mathcal{L}_e(\mathcal{G}^1_{\theta_1}) + \lambda_g\mathcal{L}_g(\mathcal{G}^1_{\theta_1},\mathcal{D}_w) + \lambda_s\mathcal{L}_s(\mathcal{G}^2_{\theta_2})
\end{equation*} 

where $\mathcal{L}_e$ is the expectation of the Euclidean distance between the colorization and the real colors, $\mathcal{L}_s$ is the expectation of the Kullback-Leibler divergence of the predicted class distribution and the VGG-16 pre-trained class distribution, both computed on the grayscale images, and $\mathcal{L}_g$ is an adversarial loss. Note that backpropagation with respect to $\mathcal{L}_s$ only affects $\mathcal{G}_{\theta_2}^2$, while backpropagation with respect to $\mathcal{L}_e$ affects the whole network.