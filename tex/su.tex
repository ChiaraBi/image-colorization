\subsection{InstColorization}
Instead of just performing learning and colorization on the entire image, InstColorization learns meaningful object-level semantics within the bounding boxes localized by an object detector. Then, we have two colorization networks: the first colorizes the whole image and the second the patches (resized to $256\times265$) in the bounding boxes. These networks have different weights but share the same architecture: the chosen architecture is the same as Zhang, as well as the loss function. Once the first networks is trained, its learned weights are used to inizialize the second network. At the end of the second network's training, the resulting full-image features and object-level features have to be combined in a consistent way by a fusion module. This allows to obtain better results on scenes with multiple objects in a cluttered background. The whole process is reported in Figure \ref{fig:su1}.

In particular, the fusion module (Figure \ref{fig:su2}) takes place at multiple layers of the colorization networks. For each layer, the full-image feature and the $N$ object-level features ($N$ is the number of detected objects) are processed by a small CNN and then combined by taking the weighted sum of the stack composed by the full-image weight map and the patches' weight maps, which have been previously reshaped (and zero padded) using the size and location of the bounding boxes for each object.


