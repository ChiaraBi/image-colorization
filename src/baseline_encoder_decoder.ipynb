{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baseline_encoder_decoder.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtVJQ4F8lv5K",
        "outputId": "de8757ff-f898-4e6c-cb9b-9585baf8a0d8"
      },
      "source": [
        "!pip install keras"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMr066EHlT9E"
      },
      "source": [
        "import keras\n",
        "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
        "from keras.preprocessing import image\n",
        "from keras.layers import Layer\n",
        "from keras.applications.inception_resnet_v2 import preprocess_input\n",
        "from keras.layers import Conv2D, UpSampling2D, InputLayer, Conv2DTranspose, Input, Reshape, merge, concatenate, Activation, Dense, Dropout, Flatten\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.callbacks import TensorBoard \n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers.core import RepeatVector, Permute\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "from skimage.color import rgb2lab, lab2rgb, rgb2gray, gray2rgb\n",
        "from skimage.transform import resize\n",
        "from skimage.io import imsave\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXUh7xRamvBk"
      },
      "source": [
        "# Get training images (resized to 256)\n",
        "X = []\n",
        "path = '/content/trainset/'\n",
        "for filename in os.listdir(path):\n",
        "    X.append(img_to_array(load_img(path+filename, target_size=(256,256), color_mode='grayscale'))) \n",
        "X = np.array(X, dtype=float)\n",
        "Xtrain = 1.0/255*X"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "KKcfBTJMqRMz",
        "outputId": "618bd5b3-6846-49c4-87d5-43803052ba33"
      },
      "source": [
        "inception = InceptionResNetV2(weights='imagenet', include_top=True)\n",
        "inception.graph = tf.get_default_graph()\n",
        "\n",
        "embed_input = Input(shape=(1000,))\n",
        "\n",
        "\n",
        "# Encoder\n",
        "encoder_input = Input(shape=(256, 256, 1,))\n",
        "encoder_output = Conv2D(64, (3,3), activation='relu', padding='same', strides=2)(encoder_input)\n",
        "encoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(encoder_output)\n",
        "encoder_output = Conv2D(128, (3,3), activation='relu', padding='same', strides=2)(encoder_output)\n",
        "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n",
        "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same', strides=2)(encoder_output)\n",
        "encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)\n",
        "encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)\n",
        "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n",
        "\n",
        "\n",
        "# Fusion\n",
        "fusion_output = RepeatVector(32 * 32)(embed_input) \n",
        "fusion_output = Reshape(([32, 32, 1000]))(fusion_output)\n",
        "fusion_output = concatenate([encoder_output, fusion_output], axis=3) \n",
        "fusion_output = Conv2D(256, (1, 1), activation='relu', padding='same')(fusion_output) \n",
        "\n",
        "\n",
        "# Decoder\n",
        "decoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(fusion_output)\n",
        "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
        "decoder_output = Conv2D(64, (3,3), activation='relu', padding='same')(decoder_output)\n",
        "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
        "decoder_output = Conv2D(32, (3,3), activation='relu', padding='same')(decoder_output)\n",
        "decoder_output = Conv2D(16, (3,3), activation='relu', padding='same')(decoder_output)\n",
        "decoder_output = Conv2D(2, (3, 3), activation='tanh', padding='same')(decoder_output)\n",
        "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
        "\n",
        "model = Model(inputs=[encoder_input, embed_input], outputs=decoder_output)\n",
        "\n",
        "\n",
        "# Create embedding\n",
        "def create_inception_embedding(grayscaled_rgb):\n",
        "    grayscaled_rgb_resized = []\n",
        "    for i in grayscaled_rgb:\n",
        "        i = resize(i, (299, 299, 3), mode='constant')\n",
        "        grayscaled_rgb_resized.append(i)\n",
        "    grayscaled_rgb_resized = np.array(grayscaled_rgb_resized)\n",
        "    grayscaled_rgb_resized = preprocess_input(grayscaled_rgb_resized)\n",
        "    with inception.graph.as_default():\n",
        "        embed = inception.predict(grayscaled_rgb_resized)\n",
        "    return embed\n",
        "\n",
        "\n",
        "# Image transformer\n",
        "datagen = ImageDataGenerator(\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        rotation_range=20,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "\n",
        "# Generate training data\n",
        "batch_size = 5\n",
        "\n",
        "def image_a_b_gen(batch_size):\n",
        "    for batch in datagen.flow(Xtrain, batch_size=batch_size):\n",
        "        grayscaled_rgb = gray2rgb(rgb2gray(batch))\n",
        "        embed = create_inception_embedding(grayscaled_rgb)\n",
        "        lab_batch = rgb2lab(batch)\n",
        "        X_batch = lab_batch[:,:,:,0]\n",
        "        X_batch = X_batch.reshape(X_batch.shape+(1,))\n",
        "        Y_batch = lab_batch[:,:,:,1:] / 128\n",
        "        yield ([X_batch, create_inception_embedding(grayscaled_rgb)], Y_batch)\n",
        "\n",
        "\n",
        "# Train model      \n",
        "tensorboard = TensorBoard(log_dir=\"YOUR OUTPUT FOLDER PATH HERE\")\n",
        "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "model.load_weights(\"color_tensorflow_real_mode.h5\")\n",
        "model.fit_generator(image_a_b_gen(batch_size), callbacks=[tensorboard], epochs=1000, steps_per_epoch=2,verbose=1) # try verbose=1 if it gives error\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Save model1\n",
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "model.save_weights(\"color_tensorflow_real_mode.h5\")\n",
        "\n",
        "\n",
        "# Make predictions on validation images\n",
        "color_me = []\n",
        "for filename in os.listdir('YOUR TESTING DATASET FOLDER PATH HERE'):\n",
        "    color_me.append(img_to_array(load_img('YOUR TESTING DATASET FOLDER PATH HERE'+filename)))\n",
        "color_me = np.array(color_me, dtype=float)\n",
        "color_me = 1.0/255*color_me\n",
        "color_me = gray2rgb(rgb2gray(color_me))\n",
        "color_me_embed = create_inception_embedding(color_me)\n",
        "color_me = rgb2lab(color_me)[:,:,:,0]\n",
        "color_me = color_me.reshape(color_me.shape+(1,))\n",
        "\n",
        "\n",
        "# Test model\n",
        "output = model.predict([color_me, color_me_embed])\n",
        "output = output * 128\n",
        "\n",
        "\n",
        "# Output colorizations\n",
        "for i in range(len(output)):\n",
        "    cur = np.zeros((256, 256, 3))\n",
        "    cur[:,:,0] = color_me[i][:,:,0]\n",
        "    cur[:,:,1:] = output[i]\n",
        "    imsave(\"result/img12_\"+str(i)+\".png\", lab2rgb(cur))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels.h5\n",
            "225214464/225209952 [==============================] - 2s 0us/step\n",
            "225222656/225209952 [==============================] - 2s 0us/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-a2e4058b38ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInceptionResNetV2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0membed_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'get_default_graph'"
          ]
        }
      ]
    }
  ]
}