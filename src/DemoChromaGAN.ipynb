{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "DemoChromaGAN_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.6.6 64-bit ('chromaGAN': virtualenv)"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "9019ed601c3bb39ceb63ab078ec9112e9e1727faf0e317e7f365960b792642c4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ChromaGAN Demo\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "This notebook allows you to colorize your images using ChromaGAN. \n",
        "\n",
        "Basic instructions:\n",
        "0.   Set GPU (Edit > Notebook settings or Runtime > Change runtime type and select GPU as Hardware accelerator.)\n",
        "1.   To execute a cell you have to select the corresponding cell by clicking on it and then click on the play icon that appears at the left top corner of the code.\n",
        "2.   To replicate the results obtained in the paper ChromaGAN execute cell number 1.\n",
        "3.   To colorize your own images execute cells number 2.1 and 2.2 in order. The execution of a cell has to end before starting the execution of the following one.\n",
        "\n",
        "If you use this demo for your research, please cite our paper [ChromaGAN: Adversarial Picture Colorization with Semantic Class Distribution](https://openaccess.thecvf.com/content_WACV_2020/papers/Vitoria_ChromaGAN_Adversarial_Picture_Colorization_with_Semantic_Class_Distribution_WACV_2020_paper.pdf):\n",
        "\n",
        "```\n",
        "\n",
        "@inproceedings{vitoria2020chromagan,\n",
        "  title={ChromaGAN: Adversarial Picture Colorization with Semantic Class Distribution},\n",
        "  author={Vitoria, Patricia and Raad, Lara and Ballester, Coloma},\n",
        "  booktitle={The IEEE Winter Conference on Applications of Computer Vision},\n",
        "  pages={2445--2454},\n",
        "  year={2020}\n",
        "}\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "7jaPdHaiS7-5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Run demo on ChromaGAN images\n",
        "\n",
        "This first part runs ChromaGAN on the images used in our paper. The results will be saved in the folder chromagan_results/ and are also displayed one by one (left: grayscale image, center: colorized image, right: ground truth image) followed by the corresponding PSNR value."
      ],
      "metadata": {
        "id": "PFJfjYH2c-Wa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip uninstall h5py --yes\n",
        "!pip install h5py==2.10.0\n",
        "\n",
        "!pip install opencv-python==4.1.25\n",
        "!pip install numpy==1.15.4\n",
        "!pip install keras==2.2.4"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "# ChromaGAN\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import tensorflow as tf\n",
        "from keras import applications\n",
        "from keras.models import load_model\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# DIRECTORY INFORMATION\n",
        "DATA_DIR = os.path.join('../img/original/ImageNet')\n",
        "OUT_DIR = os.path.join('../img/colorized/chromagan/')\n",
        "MODEL_DIR = os.path.join('../models')\n",
        "# DATA INFORMATION\n",
        "BATCH_SIZE = 1\n",
        "# TRAINING INFORMATION\n",
        "PRETRAINED = \"my_model_colorization.h5\" \n",
        "\n",
        "class DATA():\n",
        "\n",
        "    def __init__(self, dirname):\n",
        "        self.dir_path =dirname\n",
        "        self.folder_list = os.listdir(self.dir_path )\n",
        "        self.batch_size = BATCH_SIZE\n",
        "        self.size = len(self.filelist)\n",
        "        self.data_index = 0\n",
        "\n",
        "    def read_img(self, filename):\n",
        "        IMAGE_SIZE = 224\n",
        "        img = cv2.imread(filename, 3)\n",
        "        height, width, channels = img.shape\n",
        "        labimg = cv2.cvtColor(cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE)), cv2.COLOR_BGR2Lab)\n",
        "        labimg_ori = cv2.cvtColor(img, cv2.COLOR_BGR2Lab)\n",
        "        return np.reshape(labimg[:,:,0], (IMAGE_SIZE, IMAGE_SIZE, 1)), labimg[:, :, 1:], img, np.reshape(labimg_ori[:,:,0], (height, width, 1))\n",
        "\n",
        "    def generate_batch(self):\n",
        "        batch = []\n",
        "        labels = []\n",
        "        filelist = []\n",
        "        labimg_oritList= []\n",
        "        originalList = [] \n",
        "        for i in range(self.batch_size):\n",
        "            filename = os.path.join(self.dir_path, self.filelist[self.data_index])\n",
        "            filelist.append(self.filelist[self.data_index])\n",
        "            greyimg, colorimg, original, labimg_ori = self.read_img(filename)\n",
        "            batch.append(greyimg)\n",
        "            labels.append(colorimg)\n",
        "            originalList.append(original)\n",
        "            labimg_oritList.append(labimg_ori)\n",
        "            self.data_index = (self.data_index + 1) % self.size\n",
        "        batch = np.asarray(batch)/255 # values between 0 and 1\n",
        "        labels = np.asarray(labels)/255 # values between 0 and 1\n",
        "        originalList = np.asarray(originalList)\n",
        "        labimg_oritList = np.asarray(labimg_oritList)/255\n",
        "        return batch, labels, filelist, originalList, labimg_oritList\n",
        "\n",
        "def deprocess(imgs):\n",
        "    imgs = imgs * 255\n",
        "    imgs[imgs > 255] = 255\n",
        "    imgs[imgs < 0] = 0\n",
        "    return imgs.astype(np.uint8)\n",
        "\n",
        "def reconstruct(batchX, predictedY):\n",
        "    result = np.concatenate((batchX, predictedY), axis=2)\n",
        "    result = cv2.cvtColor(result, cv2.COLOR_Lab2BGR)\n",
        "         \n",
        "    return result\n",
        "\n",
        "def sample_images():\n",
        "    avg_ssim = 0\n",
        "    avg_psnr = 0\n",
        "    VGG_modelF = applications.vgg16.VGG16(weights='imagenet', include_top=True) \n",
        "    save_path = os.path.join(MODEL_DIR, PRETRAINED)\n",
        "    colorizationModel = load_model(save_path)\n",
        "    test_data = DATA(DATA_DIR)\n",
        "    assert test_data.size >= 0, \"Your list of images to colorize is empty. Please load images.\"\n",
        "    assert BATCH_SIZE<=test_data.size, \"The batch size (\" + str(BATCH_SIZE)+ \") should be smaller or equal to the number of testing images (\" + str(data_test.size)+ \") --> modify it\"\n",
        "    total_batch = int(test_data.size/BATCH_SIZE)\n",
        "    print(\"\")\n",
        "    print(\"number of images to colorize: \" + str(test_data.size))\n",
        "    print(\"total number of batches to colorize: \" + str(total_batch))\n",
        "    print(\"\")\n",
        "    if not os.path.exists(OUT_DIR):\n",
        "      print('created save result path')\n",
        "      os.makedirs(OUT_DIR)\n",
        "    for b in range(total_batch):\n",
        "            batchX, batchY, filelist, original, labimg_oritList = test_data.generate_batch()\n",
        "            predY, _ = colorizationModel.predict(np.tile(batchX,[1,1,1,3]))\n",
        "            predictVGG =VGG_modelF.predict(np.tile(batchX,[1,1,1,3]))\n",
        "            loss = colorizationModel.evaluate(np.tile(batchX,[1,1,1,3]), [batchY, predictVGG], verbose=0)\n",
        "            for i in range(BATCH_SIZE):\n",
        "                originalResult = original[i]\n",
        "                height, width, channels = originalResult.shape\n",
        "                predY_2 = deprocess(predY[i])\n",
        "                predY_2 = cv2.resize(predY_2, (width,height))\n",
        "                labimg_oritList_2 =labimg_oritList[i]\n",
        "                predResult_2= reconstruct(deprocess(labimg_oritList_2), predY_2)\n",
        "                ssim= tf.keras.backend.eval( tf.image.ssim(tf.convert_to_tensor(originalResult, dtype=tf.float32), tf.convert_to_tensor(predResult_2, dtype=tf.float32), max_val=255))\n",
        "                psnr= tf.keras.backend.eval( tf.image.psnr(tf.convert_to_tensor(originalResult, dtype=tf.float32), tf.convert_to_tensor(predResult_2, dtype=tf.float32), max_val=255))\n",
        "                avg_ssim += ssim\n",
        "                avg_psnr += psnr\n",
        "                save_path = os.path.join(OUT_DIR, \"{:.8f}_\".format(psnr)+filelist[i][:-4] +\"_reconstructed.jpg\" )\n",
        "                cv2.imwrite(save_path, predResult_2)\n",
        "                print(\"\")\n",
        "                print(\"Image \" + str(i+1) + \"/\" +str(BATCH_SIZE) + \" in batch \" + str(b+1) + \"/\" +str(total_batch) + \". From left to right: grayscale image to colorize, colorized image ( PSNR =\", \"{:.8f}\".format(psnr),\")\")\n",
        "                print(\"and ground truth image. Notice that PSNR has no sense in original black and white images.\")\n",
        "                print(\"\")\n",
        "                print(\"\")\n",
        "\n",
        "    print(\"average ssim loss =\", \"{:.8f}\".format(avg_ssim/(total_batch*BATCH_SIZE)))\n",
        "    print(\"average psnr loss =\", \"{:.8f}\".format(avg_psnr/(total_batch*BATCH_SIZE)))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    sample_images()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "number of images to colorize: 4319\n",
            "total number of batches to colorize: 4319\n",
            "\n",
            "\n",
            "Image 1/1 in batch 1/4319. From left to right: grayscale image to colorize, colorized image ( PSNR = 24.34816360 )\n",
            "and ground truth image. Notice that PSNR has no sense in original black and white images.\n",
            "\n",
            "\n",
            "\n",
            "Image 1/1 in batch 2/4319. From left to right: grayscale image to colorize, colorized image ( PSNR = 19.03439713 )\n",
            "and ground truth image. Notice that PSNR has no sense in original black and white images.\n",
            "\n",
            "\n",
            "\n",
            "Image 1/1 in batch 3/4319. From left to right: grayscale image to colorize, colorized image ( PSNR = 21.76585007 )\n",
            "and ground truth image. Notice that PSNR has no sense in original black and white images.\n",
            "\n",
            "\n",
            "\n",
            "Image 1/1 in batch 4/4319. From left to right: grayscale image to colorize, colorized image ( PSNR = 23.46192741 )\n",
            "and ground truth image. Notice that PSNR has no sense in original black and white images.\n",
            "\n",
            "\n",
            "\n",
            "Image 1/1 in batch 5/4319. From left to right: grayscale image to colorize, colorized image ( PSNR = 21.57409096 )\n",
            "and ground truth image. Notice that PSNR has no sense in original black and white images.\n",
            "\n",
            "\n",
            "\n",
            "Image 1/1 in batch 6/4319. From left to right: grayscale image to colorize, colorized image ( PSNR = 17.89464378 )\n",
            "and ground truth image. Notice that PSNR has no sense in original black and white images.\n",
            "\n",
            "\n",
            "\n",
            "Image 1/1 in batch 7/4319. From left to right: grayscale image to colorize, colorized image ( PSNR = 24.99601364 )\n",
            "and ground truth image. Notice that PSNR has no sense in original black and white images.\n",
            "\n",
            "\n",
            "\n",
            "Image 1/1 in batch 8/4319. From left to right: grayscale image to colorize, colorized image ( PSNR = 25.82670975 )\n",
            "and ground truth image. Notice that PSNR has no sense in original black and white images.\n",
            "\n",
            "\n",
            "\n",
            "Image 1/1 in batch 9/4319. From left to right: grayscale image to colorize, colorized image ( PSNR = 20.79076195 )\n",
            "and ground truth image. Notice that PSNR has no sense in original black and white images.\n",
            "\n",
            "\n",
            "\n",
            "Image 1/1 in batch 10/4319. From left to right: grayscale image to colorize, colorized image ( PSNR = 21.39401245 )\n",
            "and ground truth image. Notice that PSNR has no sense in original black and white images.\n",
            "\n",
            "\n",
            "\n",
            "Image 1/1 in batch 11/4319. From left to right: grayscale image to colorize, colorized image ( PSNR = 24.66129494 )\n",
            "and ground truth image. Notice that PSNR has no sense in original black and white images.\n",
            "\n",
            "\n",
            "\n",
            "Image 1/1 in batch 12/4319. From left to right: grayscale image to colorize, colorized image ( PSNR = 26.61254883 )\n",
            "and ground truth image. Notice that PSNR has no sense in original black and white images.\n",
            "\n",
            "\n",
            "\n",
            "Image 1/1 in batch 13/4319. From left to right: grayscale image to colorize, colorized image ( PSNR = 23.45669937 )\n",
            "and ground truth image. Notice that PSNR has no sense in original black and white images.\n",
            "\n",
            "\n",
            "\n",
            "Image 1/1 in batch 14/4319. From left to right: grayscale image to colorize, colorized image ( PSNR = 23.04047585 )\n",
            "and ground truth image. Notice that PSNR has no sense in original black and white images.\n",
            "\n",
            "\n",
            "\n",
            "Image 1/1 in batch 15/4319. From left to right: grayscale image to colorize, colorized image ( PSNR = 23.34626770 )\n",
            "and ground truth image. Notice that PSNR has no sense in original black and white images.\n",
            "\n",
            "\n",
            "\n",
            "Image 1/1 in batch 16/4319. From left to right: grayscale image to colorize, colorized image ( PSNR = 24.93112564 )\n",
            "and ground truth image. Notice that PSNR has no sense in original black and white images.\n",
            "\n",
            "\n",
            "\n",
            "Image 1/1 in batch 17/4319. From left to right: grayscale image to colorize, colorized image ( PSNR = 24.23771667 )\n",
            "and ground truth image. Notice that PSNR has no sense in original black and white images.\n",
            "\n",
            "\n",
            "\n",
            "Image 1/1 in batch 18/4319. From left to right: grayscale image to colorize, colorized image ( PSNR = 28.22375679 )\n",
            "and ground truth image. Notice that PSNR has no sense in original black and white images.\n",
            "\n",
            "\n",
            "\n",
            "Image 1/1 in batch 19/4319. From left to right: grayscale image to colorize, colorized image ( PSNR = 26.81876183 )\n",
            "and ground truth image. Notice that PSNR has no sense in original black and white images.\n",
            "\n",
            "\n",
            "\n",
            "Image 1/1 in batch 20/4319. From left to right: grayscale image to colorize, colorized image ( PSNR = 23.84516716 )\n",
            "and ground truth image. Notice that PSNR has no sense in original black and white images.\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-5ffdf3cfa398>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0msample_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-5ffdf3cfa398>\u001b[0m in \u001b[0;36msample_images\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mlabimg_oritList_2\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mlabimg_oritList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0mpredResult_2\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mreconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabimg_oritList_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredY_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mssim\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mssim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginalResult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredResult_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m                 \u001b[0mpsnr\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpsnr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginalResult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredResult_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0mavg_ssim\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mssim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.virtualenvs/chromaGAN/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1263\u001b[0m   \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m   \"\"\"\n\u001b[0;32m-> 1265\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.virtualenvs/chromaGAN/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   3162\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3164\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.virtualenvs/chromaGAN/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m     \"\"\"\n\u001b[0;32m--> 798\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mexperimental_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.virtualenvs/chromaGAN/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   5405\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5406\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 5407\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.virtualenvs/chromaGAN/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.virtualenvs/chromaGAN/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.virtualenvs/chromaGAN/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.virtualenvs/chromaGAN/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.virtualenvs/chromaGAN/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.virtualenvs/chromaGAN/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "metadata": {
        "id": "pn0KNolEdDf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f3998a6a-03c3-45a4-845c-454cdff5a48b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 Run demo on uploaded images\n",
        "\n",
        "This second part runs ChromaGAN on your selected images.\n",
        "\n",
        "## 2.1 Load images\n",
        "\n",
        "You can manually upload images from your computer. They can either be black and white images or color images. In the latter, the images are first transformed to their grayscale version and then colorized. \n",
        "\n",
        "Uploading images: first click on the folder icon located on the left. This will deploy the folders and files of the current directory. On top you'll find the upload icon that will allow you to upload all your images. These uploaded images will appear in your working directory and will later be automatically moved to the folder sample_images/.\n",
        "\n",
        "You can also directly download images from the web. For that, you have to add in the code cell below the following code line for each image to download:\n",
        "\n",
        "```\n",
        "!wget image_url\n",
        "```\n",
        "\n",
        "These images will also be saved in the current directory and will later be automatically moved to the folder sample_images/.\n",
        "\n",
        "An example for uploading one image from the web (replace this line by the lines corresponding to the images you want to download):"
      ],
      "metadata": {
        "id": "fCIKt79VoYug"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!wget https://upload.wikimedia.org/wikipedia/commons/thumb/0/00/Charlie_Chaplin.jpg/1024px-Charlie_Chaplin.jpg"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dyld: Library not loaded: /usr/local/opt/openssl/lib/libssl.1.0.0.dylib\n",
            "  Referenced from: /usr/local/bin/wget\n",
            "  Reason: image not found\n"
          ]
        }
      ],
      "metadata": {
        "id": "uYolNxuUNMFN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Run demo\n",
        "\n",
        "The results will be saved in the folder sample_results/ and are displayed one by one (left: grayscale image, center: colorized image, right: ground truth image) followed by its PSNR value. In the case of a black and white images the PSNR value has no sense since we do not have the ground truth color version to compare with."
      ],
      "metadata": {
        "id": "BoatxJSRjYwB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dyld: Library not loaded: /usr/local/opt/openssl/lib/libssl.1.0.0.dylib\n",
            "  Referenced from: /usr/local/bin/wget\n",
            "  Reason: image not found\n",
            "/bin/bash: line 1: 40396 Abort trap: 6           wget http://dev.ipol.im/~lraad/chromaGAN/model/my_model_colorization.h5\n",
            "mv: rename my_model_colorization.h5 to MODEL/my_model_colorization.h5: No such file or directory\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\n",
        "# ChromaGAN\n",
        "%tensorflow_version 1.x\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import tensorflow as tf\n",
        "from keras import applications\n",
        "from keras.models import load_model\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import math\n",
        "\n",
        "# DIRECTORY INFORMATION\n",
        "DATA_DIR = os.path.join('sample_images/')\n",
        "OUT_DIR = os.path.join('sample_results/')\n",
        "MODEL_DIR = os.path.join('MODEL/')\n",
        "# DATA INFORMATION\n",
        "BATCH_SIZE = 1\n",
        "# TRAINING INFORMATION\n",
        "PRETRAINED = \"my_model_colorization.h5\" \n",
        "\n",
        "class DATA():\n",
        "\n",
        "    def __init__(self, dirname):\n",
        "        self.dir_path =dirname\n",
        "        self.filelist = os.listdir(self.dir_path )\n",
        "        self.batch_size = BATCH_SIZE\n",
        "        self.size = len(self.filelist)\n",
        "        self.data_index = 0\n",
        "\n",
        "    def read_img(self, filename):\n",
        "        IMAGE_SIZE = 224\n",
        "        MAX_SIDE = 1500\n",
        "        img = cv2.imread(filename, 3)\n",
        "        if img is None:\n",
        "          print(\"Unable to read image: \" + filename)\n",
        "          return False, False, False, False, False\n",
        "        height, width, channels = img.shape\n",
        "        if height > MAX_SIDE or width > MAX_SIDE:\n",
        "          print(\"Image \" + filename + \" is of size (\" + str(height) + \",\" + str(width) +  \").\")\n",
        "          print(\"The maximum image size allowed is (\" + str(MAX_SIDE) + \",\" + str(MAX_SIDE) +  \").\")\n",
        "          r = min(MAX_SIDE/height,MAX_SIDE/width)\n",
        "          height = math.floor(r*height) \n",
        "          width = math.floor(r*width)\n",
        "          img = cv2.resize(img,(width,height))\n",
        "          print(\"It has been resized to (\" + str(height) + \",\" + str(width) + \")\")\n",
        "        labimg = cv2.cvtColor(cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE)), cv2.COLOR_BGR2Lab)\n",
        "        labimg_ori = cv2.cvtColor(img, cv2.COLOR_BGR2Lab)\n",
        "        return True, np.reshape(labimg[:,:,0], (IMAGE_SIZE, IMAGE_SIZE, 1)), labimg[:, :, 1:], img, np.reshape(labimg_ori[:,:,0], (height, width, 1))\n",
        "\n",
        "    def generate_batch(self):\n",
        "        batch = []\n",
        "        labels = []\n",
        "        filelist = []\n",
        "        labimg_oritList = []\n",
        "        originalList = [] \n",
        "        for i in range(self.batch_size):\n",
        "            filename = os.path.join(self.dir_path, self.filelist[self.data_index])\n",
        "            ok, greyimg, colorimg, original, labimg_ori = self.read_img(filename)\n",
        "            if ok:\n",
        "              filelist.append(self.filelist[self.data_index])\n",
        "              batch.append(greyimg)\n",
        "              labels.append(colorimg)\n",
        "              originalList.append(original)\n",
        "              labimg_oritList.append(labimg_ori)\n",
        "              self.data_index = (self.data_index + 1) % self.size\n",
        "        batch = np.asarray(batch)/255 # values between 0 and 1\n",
        "        labels = np.asarray(labels)/255 # values between 0 and 1\n",
        "        originalList = np.asarray(originalList)\n",
        "        labimg_oritList = np.asarray(labimg_oritList)/255\n",
        "        return batch, labels, filelist, originalList, labimg_oritList\n",
        "\n",
        "def deprocess(imgs):\n",
        "    imgs = imgs * 255\n",
        "    imgs[imgs > 255] = 255\n",
        "    imgs[imgs < 0] = 0\n",
        "    return imgs.astype(np.uint8)\n",
        "\n",
        "def reconstruct(batchX, predictedY):\n",
        "    result = np.concatenate((batchX, predictedY), axis=2)\n",
        "    result = cv2.cvtColor(result, cv2.COLOR_Lab2BGR)\n",
        "         \n",
        "    return result\n",
        "\n",
        "def sample_images():\n",
        "    avg_ssim = 0\n",
        "    avg_psnr = 0\n",
        "    VGG_modelF = applications.vgg16.VGG16(weights='imagenet', include_top=True) \n",
        "    save_path = os.path.join(MODEL_DIR, PRETRAINED)\n",
        "    colorizationModel = load_model(save_path)\n",
        "    test_data = DATA(DATA_DIR)\n",
        "    assert test_data.size >= 0, \"Your list of images to colorize is empty. Please load images.\"\n",
        "    assert BATCH_SIZE<=test_data.size, \"The batch size (\" + str(BATCH_SIZE)+ \") should be smaller or equal to the number of testing images (\" + str(data_test.size)+ \") --> modify it\"\n",
        "    total_batch = int(test_data.size/BATCH_SIZE)\n",
        "    print(\"\")\n",
        "    print(\"number of images to colorize: \" + str(test_data.size))\n",
        "    print(\"total number of batches to colorize: \" + str(total_batch))\n",
        "    print(\"\")\n",
        "    if not os.path.exists(OUT_DIR):\n",
        "      print('created save result path')\n",
        "      os.makedirs(OUT_DIR)\n",
        "    for b in range(total_batch):\n",
        "            batchX, batchY, filelist, original, labimg_oritList = test_data.generate_batch()\n",
        "            if batchX.any():\n",
        "              predY, _ = colorizationModel.predict(np.tile(batchX,[1,1,1,3]))\n",
        "              predictVGG =VGG_modelF.predict(np.tile(batchX,[1,1,1,3]))\n",
        "              loss = colorizationModel.evaluate(np.tile(batchX,[1,1,1,3]), [batchY, predictVGG], verbose=0)\n",
        "              for i in range(BATCH_SIZE):\n",
        "                  originalResult = original[i]\n",
        "                  height, width, channels = originalResult.shape\n",
        "                  predY_2 = deprocess(predY[i])\n",
        "                  predY_2 = cv2.resize(predY_2, (width,height))\n",
        "                  labimg_oritList_2 =labimg_oritList[i]\n",
        "                  predResult_2= reconstruct(deprocess(labimg_oritList_2), predY_2)\n",
        "                  ssim= tf.keras.backend.eval( tf.image.ssim(tf.convert_to_tensor(originalResult, dtype=tf.float32), tf.convert_to_tensor(predResult_2, dtype=tf.float32), max_val=255))\n",
        "                  psnr= tf.keras.backend.eval( tf.image.psnr(tf.convert_to_tensor(originalResult, dtype=tf.float32), tf.convert_to_tensor(predResult_2, dtype=tf.float32), max_val=255))\n",
        "                  avg_ssim += ssim\n",
        "                  avg_psnr += psnr\n",
        "                  save_path = os.path.join(OUT_DIR, \"{:.8f}_\".format(psnr)+filelist[i][:-4] +\"_reconstructed.jpg\" )\n",
        "                  cv2_imshow(np.concatenate((np.tile(labimg_oritList[i]*255,[1,1,3]), predResult_2, originalResult),axis=1))\n",
        "                  cv2.imwrite(save_path, predResult_2)\n",
        "                  print(\"\")\n",
        "                  print(\"Image \" + str(i+1) + \"/\" +str(BATCH_SIZE) + \" in batch \" + str(b+1) + \"/\" +str(total_batch) + \". From left to right: grayscale image to colorize, colorized image ( PSNR =\", \"{:.8f}\".format(psnr),\")\")\n",
        "                  print(\"and ground truth image. Notice that PSNR has no sense in original black and white images.\")\n",
        "                  print(\"\")\n",
        "                  print(\"\")\n",
        "\n",
        "    print(\"average ssim loss =\", \"{:.8f}\".format(avg_ssim/(total_batch*BATCH_SIZE)))\n",
        "    print(\"average psnr loss =\", \"{:.8f}\".format(avg_psnr/(total_batch*BATCH_SIZE)))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    sample_images()"
      ],
      "outputs": [],
      "metadata": {
        "id": "PEKuaRQRjkE6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 920
        },
        "outputId": "7ac5d455-dbb4-4686-af8f-055609600cc5"
      }
    }
  ]
}