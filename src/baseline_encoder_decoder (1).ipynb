{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baseline_encoder_decoder.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtVJQ4F8lv5K",
        "outputId": "96b0a516-8a22-43ed-be02-89dd4ebbe8d4"
      },
      "source": [
        "!pip install keras"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras==2.0.8\n",
            "  Downloading Keras-2.0.8-py2.py3-none-any.whl (276 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▏                              | 10 kB 17.2 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20 kB 19.0 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 30 kB 10.8 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 40 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████                          | 51 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 61 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 71 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 81 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 92 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 102 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 112 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 122 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 133 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 143 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 153 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 163 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 174 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 184 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 194 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 204 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 215 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 225 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 235 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 245 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 256 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 266 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 276 kB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.0.8) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.0.8) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.0.8) (1.19.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.0.8) (3.13)\n",
            "Installing collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.7.0\n",
            "    Uninstalling keras-2.7.0:\n",
            "      Successfully uninstalled keras-2.7.0\n",
            "Successfully installed keras-2.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMr066EHlT9E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "78ec6f0c-6f01-4750-ad26-371e1b611a4f"
      },
      "source": [
        "import keras\n",
        "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
        "from keras.preprocessing import image\n",
        "from keras.layers import Layer\n",
        "from keras.applications.inception_resnet_v2 import preprocess_input\n",
        "from keras.layers import Conv2D, UpSampling2D, InputLayer, Conv2DTranspose, Input, Reshape, merge, concatenate, Activation, Dense, Dropout, Flatten\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.callbacks import TensorBoard \n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers.core import RepeatVector, Permute\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "from skimage.color import rgb2lab, lab2rgb, rgb2gray, gray2rgb\n",
        "from skimage.transform import resize\n",
        "from skimage.io import imsave\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-badd8b63b49a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minception_resnet_v2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInceptionResNetV2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minception_resnet_v2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocess_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.applications.inception_resnet_v2'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip boh.zip "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSxfdDbq2mWq",
        "outputId": "fdc0752d-ec1e-463f-ca74-a0944c42824d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  boh.zip\n",
            "replace boh/Bald Eagle (1).jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: boh/Bald Eagle (1).jpg  \n",
            "  inflating: boh/Bald Eagle (2).jpg  \n",
            "  inflating: boh/Bald Eagle (3).jpg  \n",
            "  inflating: boh/Bald Eagle (4).jpg  \n",
            "  inflating: boh/Bald Eagle (5).jpg  \n",
            "  inflating: boh/Bald Eagle (6).jpg  \n",
            "  inflating: boh/Bald Eagle (7).jpg  \n",
            "  inflating: boh/Bald Eagle (8).jpg  \n",
            "  inflating: boh/Bald Eagle (9).jpg  \n",
            "  inflating: boh/Church (71).JPEG    \n",
            "  inflating: boh/Church (72).JPEG    \n",
            " extracting: boh/Church (73).JPEG    \n",
            "  inflating: boh/Church (74).JPEG    \n",
            " extracting: boh/Church (75).JPEG    \n",
            " extracting: boh/Church (76).JPEG    \n",
            "  inflating: boh/Church (77).JPEG    \n",
            "  inflating: boh/Church (78).JPEG    \n",
            "  inflating: boh/Church (79).JPEG    \n",
            "  inflating: boh/Cuban Tody (10).jpg  \n",
            "  inflating: boh/Cuban Tody (19).jpg  \n",
            "  inflating: boh/Cuban Tody (26).jpg  \n",
            "  inflating: boh/Cuban Tody (27).jpg  \n",
            "  inflating: boh/Cuban Tody (28).jpg  \n",
            "  inflating: boh/Cuban Tody (8).jpg  \n",
            "  inflating: boh/Cuban Tody (9).jpg  \n",
            " extracting: boh/English Springer (39).JPEG  \n",
            " extracting: boh/English Springer (40).JPEG  \n",
            "  inflating: boh/English Springer (41).JPEG  \n",
            "  inflating: boh/English Springer (42).JPEG  \n",
            "  inflating: boh/English Springer (47).JPEG  \n",
            "  inflating: boh/English Springer (48).JPEG  \n",
            "  inflating: boh/Fire Tailed Myzornis (24).jpg  \n",
            "  inflating: boh/Fire Tailed Myzornis (25).jpg  \n",
            "  inflating: boh/Fire Tailed Myzornis (26).jpg  \n",
            "  inflating: boh/Fire Tailed Myzornis (27).jpg  \n",
            "  inflating: boh/Fire Tailed Myzornis (28).jpg  \n",
            "  inflating: boh/Fire Tailed Myzornis (29).jpg  \n",
            "  inflating: boh/Flamingo (43).jpg   \n",
            "  inflating: boh/Flamingo (44).jpg   \n",
            "  inflating: boh/Flamingo (45).jpg   \n",
            "  inflating: boh/Flamingo (46).jpg   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip finetuning_test_.zip # rename as test"
      ],
      "metadata": {
        "id": "GONkIhtKhHhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip finetuning_train.zip # rename as train"
      ],
      "metadata": {
        "id": "qNuyWkkThWWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXUh7xRamvBk"
      },
      "source": [
        "# Get training images (resized to 256)\n",
        "X = []\n",
        "path = '/content/boh/'\n",
        "for filename in os.listdir(path):\n",
        "    X.append(img_to_array(load_img(path+filename, target_size=(256,256), color_mode='grayscale'))) \n",
        "X = np.array(X, dtype=float)\n",
        "Xtrain = 1.0/255*X"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xtrain.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3E-D7zpNpN9w",
        "outputId": "2d660abf-d53b-4c94-a85a-8e5173551f94"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(41, 256, 256, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "inception = InceptionResNetV2(weights=None, include_top=True)\n",
        "inception.load_weights('/content/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels.h5')\n",
        "'''"
      ],
      "metadata": {
        "id": "_mqr7VanjIVO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inception = InceptionResNetV2(weights='imagenet', include_top=True)\n",
        "inception.graph = tf.compat.v1.get_default_graph()\n",
        "\n",
        "inception_json = inception.to_json()\n",
        "with open(\"inception.json\", \"w\") as json_file:\n",
        "    json_file.write(inception_json)\n",
        "inception.save_weights(\"inception_weights.h5\")"
      ],
      "metadata": {
        "id": "JVqTnnTC3iPz"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "'''"
      ],
      "metadata": {
        "id": "U3ALG5-hCQBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_input = Input(shape=(1000,))\n",
        "\n",
        "# Encoder\n",
        "encoder_input = Input(shape=(256, 256, 1,))\n",
        "encoder_output = Conv2D(64, (3,3), activation='relu', padding='same', strides=2)(encoder_input)\n",
        "encoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(encoder_output)\n",
        "encoder_output = Conv2D(128, (3,3), activation='relu', padding='same', strides=2)(encoder_output)\n",
        "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n",
        "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same', strides=2)(encoder_output)\n",
        "encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)\n",
        "encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)\n",
        "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n",
        "\n",
        "\n",
        "# Fusion\n",
        "fusion_output = RepeatVector(32 * 32)(embed_input) \n",
        "fusion_output = Reshape(([32, 32, 1000]))(fusion_output)\n",
        "fusion_output = concatenate([encoder_output, fusion_output], axis=3) \n",
        "fusion_output = Conv2D(256, (1, 1), activation='relu', padding='same')(fusion_output) \n",
        "\n",
        "\n",
        "# Decoder\n",
        "decoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(encoder_output)\n",
        "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
        "decoder_output = Conv2D(64, (3,3), activation='relu', padding='same')(decoder_output)\n",
        "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
        "decoder_output = Conv2D(32, (3,3), activation='relu', padding='same')(decoder_output)\n",
        "decoder_output = Conv2D(16, (3,3), activation='relu', padding='same')(decoder_output)\n",
        "decoder_output = Conv2D(2, (3, 3), activation='tanh', padding='same')(decoder_output)\n",
        "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
        "\n",
        "model = Model(inputs=[encoder_input, embed_input], outputs=decoder_output)"
      ],
      "metadata": {
        "id": "E16xuxDNjPOz"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_inception_embedding(grayscaled_rgb):\n",
        "    grayscaled_rgb_resized = []\n",
        "    for i in grayscaled_rgb:\n",
        "        i = resize(i, (299, 299, 3), mode='constant')\n",
        "        grayscaled_rgb_resized.append(i)\n",
        "    grayscaled_rgb_resized = np.array(grayscaled_rgb_resized)\n",
        "    grayscaled_rgb_resized = preprocess_input(grayscaled_rgb_resized)\n",
        "    with inception.graph.as_default():\n",
        "      json_file = open('inception.json','r')\n",
        "      loaded_model_json = json_file.read()\n",
        "      json_file.close()\n",
        "      loaded_model = tf.keras.models.model_from_json(loaded_model_json)\n",
        "      #load weights into new model\n",
        "      #loaded_model.load_weights(\"color_tensorflow_real_mode.h5\")\n",
        "      loaded_model.compile(loss='mse',optimizer='adam',metrics=['accuracy'])\n",
        "      embed = loaded_model.predict(grayscaled_rgb_resized)\n",
        "    return embed\n",
        "\n",
        "def image_a_b_gen(batch_size, Xtrain, datagen):\n",
        "    for batch in datagen.flow(Xtrain, batch_size=batch_size):\n",
        "        grayscaled_rgb = gray2rgb(batch)\n",
        "        grayscaled_rgb = grayscaled_rgb.reshape(grayscaled_rgb.shape[0], 256, 256, 3)\n",
        "        embed = create_inception_embedding(grayscaled_rgb)\n",
        "        lab_batch = rgb2lab(grayscaled_rgb)\n",
        "        X_batch = lab_batch[:,:,:,0]\n",
        "        X_batch = X_batch.reshape(X_batch.shape+(1,))\n",
        "        Y_batch = lab_batch[:,:,:,1:] / 128\n",
        "        # return one value at time\n",
        "        yield ([X_batch, create_inception_embedding(grayscaled_rgb)], Y_batch)"
      ],
      "metadata": {
        "id": "j-B5uZ_UjV1u"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Image transformer\n",
        "datagen = ImageDataGenerator(\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        rotation_range=20,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "# Generate training data\n",
        "batch_size = 5"
      ],
      "metadata": {
        "id": "QsUhV6wxlUNV"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model      \n",
        "tensorboard = TensorBoard(log_dir=\"colorized\")\n",
        "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "#model.load_weights(\"color_tensorflow_real_mode1.h5\")\n",
        "model.fit(image_a_b_gen(batch_size, Xtrain, datagen), callbacks=[tensorboard], epochs=1000, steps_per_epoch=2,verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "id": "y6gwlUeUjs1N",
        "outputId": "560688d2-11fe-4faa-d042-caf1005d9a33"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-5b24a226b162>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#model.load_weights(\"color_tensorflow_real_mode1.h5\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_a_b_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatagen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1200\u001b[0m       \u001b[0;31m# happen after `callbacks.on_train_begin`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m       data_handler._initial_epoch = (  # pylint: disable=protected-access\n\u001b[0;32m-> 1202\u001b[0;31m           self._maybe_load_initial_epoch_from_ckpt(initial_epoch))\n\u001b[0m\u001b[1;32m   1203\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: _maybe_load_initial_epoch_from_ckpt() missing 1 required positional argument: 'mode'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "model.save_weights(\"color_tensorflow_real_mode1.h5\")"
      ],
      "metadata": {
        "id": "mgLk4C3AtUsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_file = open('model.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "loaded_model = tf.keras.models.model_from_json(loaded_model_json)\n",
        "json_file.close()\n",
        "loaded_model.save_weights(\"color_tensorflow_real_mode.h5\")"
      ],
      "metadata": {
        "id": "cxTNbKr9FaZM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_inception_embedding(grayscaled_rgb):\n",
        "    grayscaled_rgb_resized = []\n",
        "    for i in grayscaled_rgb:\n",
        "        i = resize(i, (299, 299, 3), mode='constant')\n",
        "        grayscaled_rgb_resized.append(i)\n",
        "    grayscaled_rgb_resized = np.array(grayscaled_rgb_resized)\n",
        "    grayscaled_rgb_resized = preprocess_input(grayscaled_rgb_resized)\n",
        "    with inception.graph.as_default():\n",
        "        json_file = open('inception.json','r')\n",
        "        loaded_model_json = json_file.read()\n",
        "        json_file.close()\n",
        "        loaded_model = tf.keras.models.model_from_json(loaded_model_json)\n",
        "        #load weights into new model\n",
        "        #loaded_model.load_weights(\"color_tensorflow_real_mode.h5\")\n",
        "        loaded_model.compile(loss='mse',optimizer='adam',metrics=['accuracy'])\n",
        "        embed = inception.predict(grayscaled_rgb_resized)\n",
        "    return embed"
      ],
      "metadata": {
        "id": "9EEOHhvhNCgv"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_inception_embedding(grayscaled_rgb):\n",
        "    grayscaled_rgb_resized = []\n",
        "    for i in grayscaled_rgb:\n",
        "        i = resize(i, (299, 299, 3), mode='constant')\n",
        "        grayscaled_rgb_resized.append(i)\n",
        "    grayscaled_rgb_resized = np.array(grayscaled_rgb_resized)\n",
        "    grayscaled_rgb_resized = preprocess_input(grayscaled_rgb_resized)\n",
        "    with inception.graph.as_default():\n",
        "      json_file = open('inception.json','r')\n",
        "      loaded_model_json = json_file.read()\n",
        "      json_file.close()\n",
        "      loaded_model = tf.keras.models.model_from_json(loaded_model_json)\n",
        "      #load weights into new model\n",
        "      #loaded_model.load_weights(\"color_tensorflow_real_mode.h5\")\n",
        "      loaded_model.compile(loss='mse',optimizer='adam',metrics=['accuracy'])\n",
        "      embed = loaded_model.predict(grayscaled_rgb_resized)\n",
        "    return embed"
      ],
      "metadata": {
        "id": "zuMAzNEmOuDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "id": "KKcfBTJMqRMz",
        "outputId": "c9818b94-4e03-479e-fcf6-c622622beff9"
      },
      "source": [
        "json_file = open('model.json', 'r')\n",
        "loaded_model2_json = json_file.read()\n",
        "loaded_model2 = tf.keras.models.model_from_json(loaded_model2_json)\n",
        "json_file.close()\n",
        "loaded_model2.load_weights(\"color_tensorflow_real_mode.h5\")\n",
        "\n",
        "\n",
        "# Make predictions on validation images\n",
        "color_me = []\n",
        "for filename in os.listdir(path):\n",
        "    color_me.append(img_to_array(load_img(path+filename, target_size=(256,256), color_mode='grayscale'))) \n",
        "print(color_me[0].shape)\n",
        "color_me = np.array(color_me, dtype=float)\n",
        "print(color_me.shape)\n",
        "color_me = 1.0/255*color_me\n",
        "grayscaled_rgb = gray2rgb(color_me)\n",
        "grayscaled_rgb = grayscaled_rgb.reshape(grayscaled_rgb.shape[0], 256, 256, 3)\n",
        "color_me_embed = create_inception_embedding(grayscaled_rgb)\n",
        "color_me = rgb2lab(grayscaled_rgb)\n",
        "color_me = color_me[:,:,:,0]\n",
        "color_me = color_me.reshape(color_me.shape+(1,))\n",
        "\n",
        "\n",
        "# Test model\n",
        "output = loaded_model2.predict([color_me, color_me_embed])\n",
        "output = output * 128\n",
        "\n",
        "\n",
        "# Output colorizations\n",
        "for i in range(len(output)):\n",
        "    cur = np.zeros((256, 256, 3))\n",
        "    cur[:,:,0] = color_me[i][:,:,0]\n",
        "    cur[:,:,1:] = output[i]\n",
        "    imsave(\"result/img12_\"+str(i)+\".png\", lab2rgb(cur))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(256, 256, 1)\n",
            "(41, 256, 256, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-3a15257d9dda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Test model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_model2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolor_me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor_me_embed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    963\u001b[0m             \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ma\u001b[0m \u001b[0mmultiple\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m     \"\"\"\n\u001b[0;32m--> 965\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_built_as_v1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0mbase_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_api_gauge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_call_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer_v1.py\u001b[0m in \u001b[0;36m_assert_built_as_v1\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    832\u001b[0m           \u001b[0;34m' custom __init__ which didn\\'t call super().__init__. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m           \u001b[0;34m' Please check the implementation of %s and its bases.'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m           (type(self),))\n\u001b[0m\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Your Layer or Model is in an invalid state. This can happen for the following cases:\n 1. You might be interleaving estimator/non-estimator models or interleaving models/layers made in tf.compat.v1.Graph.as_default() with models/layers created outside of it. Converting a model to an estimator (via model_to_estimator) invalidates all models/layers made before the conversion (even if they were not the model converted to an estimator). Similarly, making a layer or a model inside a a tf.compat.v1.Graph invalidates all layers/models you previously made outside of the graph.\n2. You might be using a custom keras layer implementation with  custom __init__ which didn't call super().__init__.  Please check the implementation of <class 'keras.engine.functional.Functional'> and its bases."
          ]
        }
      ]
    }
  ]
}